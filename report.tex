\documentclass[twocolumn]{article}
% \documentclass[a4paper]{article}
\usepackage{color}
\usepackage[top=1in,bottom=1in,left=1.2in,right=1.2in]{geometry}
\usepackage{hyperref}
\usepackage[small]{titlesec}

\usepackage{amsmath}

\usepackage[]{minted}
\newenvironment{code}{\captionsetup{type=listing}}

\newcommand{\todo}[1]{\textcolor{blue}{\textbf{TODO:} #1}}
\newcommand{\red}[1]{\textcolor{red}{\textbf{Note:} #1}}

\title{CS6320: Final Project Report \\ \begin{small}\url{https://github.com/COftedahl/cs6320_group_project}\end{small}}
\author{Brendan Martel \\ BXM240013
    \and Cole Oftedahl \\ CXO220001
    \and Brad Stover \\ BES170230
    \and Zhaotong Zhang \\ ZXZ220016}
\date{}

\begin{document}
\maketitle




\section{Introduction}

% \todo{The ﬁrst paragraph should brieﬂy describe task and data.}
% \todo{The second paragraph should brieﬂy describe your approach. Try to motivate it.}
% \todo{The third paragraph should describe your main experiments and results, including mentioning the data you use.}
Sentiment analysis aspires to determine the emotional polarity expressed in natural language and is typically categorized into three classes: positive, negative, and neutral. Although this task has continuously been widely explored, social media platforms--especially Twitter--bring up unique challenges. The language in social media is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and \#hashtags1[1].  In order to train and evaluate our project, we combine several publicly available tweet datasets, including Sentiment140 and two additional tweet corpora spanning multiple years and topics. Gathering these datasets did yield an extensive and heterogeneous collection suitable for this experiment.

With the growing availability of large-scale Tweet datasets spanning several years and different topics, it has become increasingly critical to develop sentiment classification systems. These systems not only perform well on small, homogeneous datasets; they also scale to enormous, heterogeneous corpora. Modern transformer-based language models, such as BERT and RoBERTa, have demonstrated powerful performance in many NLP tasks due to their contextual understanding. Classical lexicon-based tools, such as VADER, remain widely used for real-time or resource-constrained sentiment inference. In recent years, large-language models (LLMs), such as LLaMA, have made it feasible to fine-tune powerful generative architectures on domain-specific tasks, enabling improvements when working with massive datasets.

Using the combined Tweet datasets from several sources, we train and evaluate several models on consistent training, validation, and testing splits. Our results show that transformer models outperform the classical baseline, with Sentiment-BERT exhibiting the strongest performance. The transformer models demonstrate an advantage in capturing subtle sentiment features in large and diverse Tweet corpora. These findings emphasize the importance of both dataset scale and model architecture/capacity in Twitter sentiment classification. 


% \section{Task (5pt)}
% \todo{Define the task. -- Specify the inputs and outputs. This includes your notation. Section 2 of this paper~\cite{du-etal-2021-qa} may give you an idea.}


\section{Data}

% \todo{Describe the data you use, including how many examples are in the training, development, and test sets.}
In this project, we formed a sentiment classification system using a massive combined Twitter corpus derived from multiple public datasets. We parsed and normalized data from Sentiment140, Twitter Tweets Sentiment Dataset, Sentiment Analysis of Tweets dataset, and additional labeled Tweet collections. After doing this, we gathered and unified all entries into a consistent three-class sentiment format consisting of positive, neutral, and negative sentiments. Then we applied our preprocessing pipeline, which cleaned noise, removed URLs, and standardized tokens. In the meantime, we split the resulting corpus into three equal partitions for training, validation, and testing. 
\begin{table}[h!]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Dataset Split} & \textbf{Number of Examples} \\
\hline
Training Set     &  \texttt{1,329,985} \\
Validation Set  &  \texttt{168,748} \\
Test Set         &  \texttt{168,748} \\
\hline
\end{tabular}
\caption{Number of examples in the training, validation, and test sets.}
\label{tab:dataset_splits}
\end{table}


\section{Methodology}

% \red{If your project is proposing a new model/method.}

% \todo{Describe your model. If you use a neural network, you should define the architecture. There's no need to write equations of common units (e.g., LSTM), so you can just use functions to refer to using such units. If you use linear models with features, provide details.}
% 
% Briefly describe what learning algorithm you are using. Specify your learning objective;
% 
% Briefly Describe your inference procedure. This is the process you use to make predictions during testing.}

% \red{If your project is more analysis-based and propose how to analyze model behaviors/outputs}

% \todo{Section 4 of \cite{das-etal-2022-automatic} is an example. If you do manual inspections, please provide the details such as analysis/annotation guidelines.}
% In this section, we explain the data preparation pipeline, model training procedures, evaluation setup, and manual analysis framework used to study and analyze model behaviors.  Our methodology follows the general structure of Section 4 in Das et al. (2022), which focuses on automated metrics and on qualitative inspection and error categorization to better understand and learn from the model outputs.

\subsection{Data Sources and Integration}

To create a general-purpose sentiment classification corpus, we aggregated three publicly available tweet-based datasets:

\begin{itemize} \small
    \item \textbf{Sentiment\_Analysis.csv}: A collection of labeled English posts annotated for positive, negative, or neutral sentiment.
    \item \textbf{Sentiment140.csv}: A large-scale Twitter dataset containing polarity-annotated tweets collected across multiple years.
    \item \textbf{Tweets.csv}: A mixed-source Twitter dataset containing short and informal posts covering diverse subjects.
\end{itemize}

Even though these datasets originate from different time periods and subject domains, their sentiment labels were standardized to a unified three-class scheme:

{\small
\begin{itemize}
    \item \textbf{Positive} -- 2
    \item \textbf{Neutral} -- 1
    \item \textbf{Negative} -- 0
\end{itemize}
}


This merging procedure allows us to form a wide, diverse, and domain-agnostic sentiment corpus suitable for evaluating the robustness of modern language models.

\subsection{Data Preprocessing and Normalization}
\label{sec:preprocessing}

Before building the final training, validation, and testing splits, all raw data files underwent a unified preprocessing stage:
{\small
\begin{itemize}
    \item \textbf{URL and username removal:}  
    Links such as \texttt{http://...} and user handles like \texttt{@name} were stripped using the \texttt{cleanLine} function; @'s were replaced by a token.

    \item \textbf{Consistent text formatting:}  
    All text was converted to lowercase and cleaned from excessive whitespace or punctuation.

    \item \textbf{Character filtering:}  
    Non-English characters and redundant punctuation marks were removed or collapsed.

    \item \textbf{Label normalization:}  
    Dataset-specific labels (e.g., $-1$, $0$, $1$) were normalized to the range $0$--$2$ through a constant shift.
\end{itemize}
}

After preprocessing, each dataset was written in the format:
\[
\{ \text{sentence}, \text{label} \}.
\]

The \texttt{splitData()} routine was then executed to construct the final splits:
{\small
\begin{itemize}
    \item \textbf{80\% Training set:} \texttt{train.csv}
    \item \textbf{10\% Validation set:} \texttt{val.csv}
    \item \textbf{10\% Test set:} \texttt{test.csv}
\end{itemize}
}

This split guaranteed that all models were trained and evaluated on the same unified distribution.

% ---------------------------------------------------------

\subsection{Model Architecture and Training Procedure}
\label{sec:models}

The project primarily experimented with transformer-based architectures due to their state-of-the-art performance in text classification. The implemented and planned models include:
{\small
\begin{itemize}
    \item \textbf{RoBERTa Twitter:}
    \texttt{cardiffnlp/\allowbreak twitter-roberta-base-sentiment-latest}

    \item \textbf{BERT-based sentiment models} from HuggingFace

    \item \textbf{VADER:} a lexicon-based baseline
\end{itemize}
}

\subsubsection*{Training Pipeline}

All transformer models are fine-tuned using HuggingFace’s \texttt{Trainer} with the following defaults:
{\small
\begin{itemize}
    \item \textbf{Loss:} Cross-entropy
    \item \textbf{Optimizer:} AdamW
    \item \textbf{Batch size:} 8
    \item \textbf{Epochs:} 2--3
    \item \textbf{Evaluation metric:} Confusion matrix using \texttt{evaluate.load("confusion\_matrix")}
    \item \textbf{Tokenization:} \texttt{AutoTokenizer} with truncation to 256 tokens
\end{itemize}
}

The same training/validation/testing splits were used for all models in order to ensure a fair comparison across different architectures.

VADER, as a non-transformer, was used as a baseline against which to evaluate the transformer models. In this study, VADER was not trained, but it was used to provide baseline classification scores by being run on a concatenation of all the datasets and evaluating its performance. 

\section{Implementations}

% \todo{Explain your implementation for the project (e.g. which commands/algorithms/data structures you used). What existing packages are used?
% When describing your implementation details (e.g. decoding), we suggest that you include corresponding but only \textbf{important} code pieces (e.g. classes, data structures, algorithms. It may be in the form of a screenshot or latex source code like in Listing~\ref{lst:eg}).
% }

The transformer models were implemented using the HuggingFace transformers library in Python. Since multiple different models were evaluated, abstractions were used to allow simple reuse by passing a different parameter to indicate which model to use. 

To this end, the LanguageModel class was developed, incorporating a constructor method as well as training and testing methods. Development of this class was largely based on the documentation available from HuggingFace, and thus the class closely follows example code from the provided tutorials. 

\begin{listing}[H]
\begin{minted}[frame=single,framesep=10pt]{python}
  def test(self, data): 
    return self.pipe(data)
\end{minted}
\caption{LanguageModel test() method}
\label{lst:eg}
\end{listing}

The constructor method simply accepts the model name as a parameter and then initializes variables to store the necessary model details used for training and testing. The test method is very simple, composed of only one line, listed above. The data parameter is the prompt being evaluated, and the simple interface provided by the transformers library allows calling pipe() with the data to compute the result using the loaded language model. 

\begin{listing}[H]
\begin{minted}[frame=single,framesep=10pt]{python}
def train(self, trainData, 
  trainingArgs
): 
  tokenizedTrainData = trainData.map(
    tokenizeDataset, batched=True)
  metric = evaluate.load(
    "confusion_matrix")
  trainer = Trainer(
    model=self.model,
    args=trainingArgs,
    train_dataset=tokenizedTrainData,
    eval_dataset=tokenizedTrainData,
    compute_metrics=compute_metrics,
  )
  trainer.train()
\end{minted}
\caption{LanguageModel train() method}
\label{lst:eg}
\end{listing}

The train method is more complex, requiring setup to utilize the provided Trainer interface from the transformers library. First, the method accepts the training dataset "trainData", and then accepts "trainingArgs" to define parameters for the training. To implement the training, the Trainer class is used, which requires the training data to be tokenized and also requires providing compute metrics, which allow the model to accurately compute the loss for each pass. 

\begin{listing}[H]
\begin{minted}[frame=single,framesep=10pt]{python}
  def tokenizeDataset(dataset):
    return self.tokenizer(
      dataset[TEXT_COLUMN_NAME], 
      padding="max_length", 
      truncation=True
    )
  tokenizedTrainData = trainData.map(
    tokenizeDataset, batched=True
  )
\end{minted}
\caption{Tokenizing data in train() method}
\label{lst:eg}
\end{listing}

The above listing details how the datasets are tokenized. This method is called to map "trainData" to "tokenizedTrainData", which ensures each input sent to the language model is of the proper size by using padding and truncation. 

\begin{listing}[H]
\begin{minted}[frame=single,framesep=10pt]{python}
  metric = evaluate.load(
    "confusion_matrix")

  def compute_metrics(eval_pred):
    logits, labels = eval_pred
    # convert the logits to 
    # their predicted class
    predictions = np.argmax(logits, 
      axis=-1)
    return metric.compute(
      predictions=predictions, 
      references=labels
    )
\end{minted}
\caption{Defining training metrics in train() method}
\label{lst:eg}
\end{listing}

Finally, the metrics must be provided to the Trainer to allow it to evaluate its performance during training. Using the confusion matrix approach, the predictions are checked against the reference labels evaluate the performance of the model in the training cycle. 

The other portion of code written specifically for language processing tasks was the use of VADER. For this, the Python package vaderSentiment was used, specifically utilizing the SentimentIntensityAnalyzer for computing a baseline sentiment prediction for the datasets. Much of this code is also based on the documentation of the vaderSentiment package. 

\begin{listing}[H]
\begin{minted}[frame=single,framesep=10pt]{python}
analyzer = SentimentIntensityAnalyzer()
predictions = []
for sentence in tqdm(trainingData[
    TEXT_COLUMN_NAME]):
  vs = analyzer.polarity_scores(
      sentence)
  label = getOverallSentiment(vs)
  predictions.append(label)
metrics = computeMetrics(
    predictions, trainingData)
\end{minted}
\caption{Main VADER Evaluation Loop}
\label{lst:eg}
\end{listing}

VADER provides most functionality already, with each call of the "polarity\_scores" method returning the probability of the given input belonging to each of the three possible classifications, along with an aggregration of those probabilities in a field called "compound". Thus, the program simply calls this method on each input sentence to get the classification, storing the results in an array to later be passed to a helper method for computing metrics of the model. However, since the "polarity\_scores" method returns an object with multiple classification scores, the method "getOverallSentiment" was developed to classify the sentiment using the returned compound score. 

\begin{listing}[H]
\begin{minted}[frame=single,framesep=10pt]{python}
THRESHOLD_VALUE = 0.5
POSITIVE_THRESHOLD = THRESHOLD_VALUE
NEGATIVE_THRESHOLD = -THRESHOLD_VALUE
def getOverallSentiment(sentiment): 
  if sentiment['compound'] >= 
      POSITIVE_THRESHOLD:
    return SENTIMENT_RESULT_ENUM.
        POSITIVE
  elif sentiment['compound'] <= 
      NEGATIVE_THRESHOLD:
    return SENTIMENT_RESULT_ENUM.
        NEGATIVE
  else:
    return SENTIMENT_RESULT_ENUM.
        NEUTRAL
\end{minted}
\caption{VADER Sentiment Classification}
\label{lst:eg}
\end{listing}

The above listing details the thresholds used for classifications, which are based on the thresholds used in the documentation. Based on the compound score, a negative score below the threshold is classified as negative sentiment, a positive score above the threshold as positive sentiment, and any other score as neutral. 

\section{Experiments and Results}

% \red{If your project is proposing a new model/method.}
% \paragraph{Development and Test Results}
% \todo{Briefly explain here how you evaluated the models and metrics used. 
% Put the results into clear tables or diagrams and include your observations and analysis.}

% \paragraph{Error Analysis}
% \todo{Qualitative analysis of selected failure examples. Show and discuss error examples from your development set. Identify certain classes of errors and use the examples to illustrate them. 
% Please analyze with respect to unknown words as well.
% }

% \paragraph{(Bonus +2pt) Speed Analysis}
% \todo{Speed analysis and computation needs are a major issue of the task. Makes sure to report the costs of inference per example.}

% \red{If your project is more analysis-based and propose how to analyze model behaviors/outputs}

% \todo{Please refer to Section 7 Experimental Results and Analysis in this paper, \url{https://aclanthology.org/2022.acl-long.274.pdf}. Basicly, you provide comprehensive analysis of model outputs, error cases, comparisons between model outputs and insights/obersations that can be derived.}

\subsection{Overall Model Performance}

Table~\ref{tab:overall_performance} summarizes the accuracy, macro-averaged
precision, recall, and F1 scores for all systems.  
The numerical values are imported from \texttt{table.tex}.

\begin{table*}[t]
\centering
\caption{Model Performance on Test Set}
\label{tab:overall_performance}
\begin{tabular}{lcccc}
\hline
\textbf{Model} & \textbf{Acc.} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} \\
\hline
BERT Multilingual & 0.7232 & 0.7529 & 0.7166 & 0.7252 \\
Sentiment-BERT & 0.7712 & 0.7974 & 0.7648 & 0.7737 \\
Twitter-RoBERTa & 0.7568 & 0.7728 & 0.7540 & 0.7604 \\
VADER & 0.6746 & 0.0244 & 0.3483 & 0.0455 \\
\hline
\end{tabular}
\end{table*}

VADER performed the worst of all the models, which was expected, as it does not follow the transformer architecture and also because VADER is not trained on the Twitter data before testing (like the transformer models are). 
Sentiment-BERT achieved the best overall results with 77.12\% accuracy and 0.7737
macro F1, outperforming both BERT Multilingual and Twitter-RoBERTa.
This aligns with expectations: Sentiment-BERT is explicitly trained for sentiment
classification, whereas BERT Multilingual supports broader multilingual tasks,
and Twitter-RoBERTa is optimized for tweet semantics but not solely sentiment.

\subsection{Class-Level Performance}

Across every model, the neutral class consistently obtained the highest recall but the lowest precision. It reflects the dataset’s inherent ambiguity and the linguistic overlap between mild positive, mild negative, and neutral tweets.

\vspace{-1.2em}
\paragraph{BERT Multilingual}
{\small
\begin{itemize}
    \item Positive precision: 0.8599 (strongest)
    \item Negative recall: 0.6703 (weakest)
    \item Frequently mislabels mild sentiment as Neutral
    \item Confusion matrix reveals strong spillover from Positive $\rightarrow$ Neutral with 60 instances
\end{itemize}
}

\vspace{-1.2em}
\paragraph{Sentiment-BERT}
{\small
\begin{itemize}
    \item Highest precision, recall, and F1 across all classes
    \item Best detection of Neutral sentiment with Recall is 0.8432
    \item More robust separation between Positive and Negative
    \item Lower misclassification rate overall as shown in its confusion matrix
\end{itemize}
}

\vspace{-1.2em}
\paragraph{Twitter-RoBERTa}
{\small
\begin{itemize}
    \item Strong Positive Precision - 0.84
    \item Strong Negative Precision - 0.80
    \item More balanced performance than BERT Multilingual
    \item Slightly inferior to Sentiment-BERT on Neutral recall
    \item Tends to confuse borderline Negative/Neutral cases
\end{itemize}
}

These patterns imply that pretraining domain specificity--including Sentiment datasets vs. general-purpose vs. Twitter corpora--has a measurable impact on classification behaviour.

\subsection{Confusion Matrix Analysis}


\paragraph{BERT Multilingual}
{\small
\begin{itemize}
    \item 54 cases: Neutral mispredictions for Negative
    \item 60 cases: Neutral mispredictions for Positive
    \item Strong preference to classify ambiguous emotions as Neutral
    \item Implies conservative sentiment boundaries and difficulty with informal tone
\end{itemize}}

\vspace{-1.2em}
\paragraph{Sentiment-BERT}
{\small
\begin{itemize}
    \item Cleanest separation among classes
    \item Very low cross-polarity confusion, such as 6 cases: Positive $\rightarrow$ Negative
    \item Strong Neutral recall with 199 cases correct, indicating better understanding of ambiguous expressions
\end{itemize}}

\vspace{-1.2em}
\paragraph{Twitter-RoBERTa}
{\small
\begin{itemize}
    \item Balanced overall but less stable than Sentiment-BERT
    \item Higher Negative $\leftrightarrow$ Neutral swaps: 49 cases and 26 cases
    \item Robust Positive sentiment identification, with 156 cases correct
\end{itemize}}


\subsection{Error Analysis}

Common sources of error across all models include:
{\small
\begin{itemize}
    \item \textbf{Sarcasm and irony} \\
    ``yeah amazing job\ldots'' --- actually negative, but predicted as neutral.  
    Models lack explicit sarcasm-handling ability.

    \item \textbf{Implicit sentiment} \\
    ``wish the app worked better'' --- expresses negative intent without explicit emotional keywords.

    \item \textbf{Mixed sentiment} \\
    ``love the screen, hate the battery life''

    \item \textbf{Short or context-poor tweets} \\
    Single-token posts such as ``great'', ``wow'', ``ok'', where polarity depends heavily on external context.

    \item \textbf{Lexical ambiguity} \\
    Words like \textit{fine}, \textit{ok}, and \textit{sick} vary in meaning by speaker and domain.
\end{itemize}}

These patterns match prior findings in Twitter sentiment modeling literature and align with the analysis trends in Section~7 of the cited ACL 2022 paper.

\subsection{Comparison Across Models}

A consistent ranking emerges across all metrics:

{\small
\begin{itemize}
    \item \textbf{Sentiment-BERT (best)}
    \item \textbf{Twitter-RoBERTa}
    \item \textbf{BERT Multilingual (worst)}
\end{itemize}}

\vspace{-1.2em}
\paragraph{Interpretation:}

{\small
\begin{itemize}
    \item \textbf{Sentiment-BERT} benefits from pretraining directly on sentiment corpora, leading to superior generalization.
    \item \textbf{Twitter-RoBERTa} benefits from social media text modeling but lacks explicit polarity supervision.
    \item \textbf{BERT Multilingual} is trained on multilingual data, and its sentiment head is less aligned with informal, sarcastic, or domain-specific text.
\end{itemize}}

This demonstrates that \textbf{pretraining domain and task alignment are essential} for effective sentiment classification.

\subsection{Summary of Findings}

{\small
\begin{itemize}
    \item \textbf{Sentiment-BERT} provides the most accurate and stable performance, achieving \textbf{0.7712 accuracy} and \textbf{0.7737 macro F1}.
    \item \textbf{Twitter-RoBERTa} performs competitively but struggles with ambiguity around the Neutral boundary.
    \item \textbf{BERT Multilingual} displays the weakest polarity distinctions.
    \item The \textbf{Neutral class remains the primary source of error} across all models.
    \item \textbf{Dataset heterogeneity} increases robustness yet simultaneously introduces more ambiguity.
    \item Error patterns such as \textit{sarcasm, implicit sentiment, and mixed polarity} align with challenges documented in prior research.
\end{itemize}}


\section{Conclusion}
% \todo{Brief conclusion summarizing findings (from both numerical results and qualitative analysis).}

Sentiment analysis can be performed to a high degree of accuracy using modern technologies such as large-scale transformers trained to be attuned to the specific task, while older or non-trained styles of language models fail to achieve the same accuracy in their performance. However, even when using these modern models, difficulties present in the nuance of natural language--particularly in a realm of less grammatically-structured text (as in social media)--can cause inaccuracies in sentiment classification. Additionally, further work may need to be done to improve the ability of models to identify neutral sentiment, which posed the greatest challenge. 

For most sentiment classification tasks, transformer models such as BERT and RoBERTa will achieve sufficient accuracy given they are trained on a sufficiently large and quality dataset. While other model architectures are usable, particularly in resource-constrained contexts, transformer models remain the standard in sentiment analysis performance. 

\clearpage
\section{Bibliography}
\bibliographystyle{plain}
\bibliography{custom}
[1] S. Rosenthal, P. Nakov, A. Ritter, and V. Stoyanov, “SemEval-2014 Task 9: Sentiment Analysis in Twitter,” 2014.

[2]A. Das et al., “Automatic Error Analysis for Document-level Information Extraction,” vol. 1, pp. 3960–3975, 2022, Accessed: Nov. 29, 2025. [Online]. Available: https://aclanthology.org/2022.acl-long.274.pdf


\end{document}

\documentclass[twocolumn]{article}
% \documentclass[a4paper]{article}
\usepackage{color}
\usepackage[top=1in,bottom=1in,left=1.2in,right=1.2in]{geometry}
\usepackage{hyperref}
\usepackage[small]{titlesec}

\usepackage{amsmath}

\usepackage[]{minted}
\newenvironment{code}{\captionsetup{type=listing}}

\newcommand{\todo}[1]{\textcolor{blue}{\textbf{TODO:} #1}}
\newcommand{\red}[1]{\textcolor{red}{\textbf{Note:} #1}}

\title{CS6320: Final Project Report \\ \begin{small}\url{https://github.com/COftedahl/cs6320_group_project}\end{small}}
\author{Brendan Martel \\ BXM240013
    \and Cole Oftedahl \\ CXO220001
    \and Bradley Evan Stover \\ BES170230
    \and Zhaotong Zhang \\ ZXZ220016}
\date{}

\begin{document}
\maketitle




\section{Introduction (10pt)}

\todo{The ﬁrst paragraph should brieﬂy describe task and data.}
\todo{The second paragraph should brieﬂy describe your approach. Try to motivate it.}
\todo{The third paragraph should describe your main experiments and results, including mentioning the data you use.}
Sentiment analysis aspires to determine the emotional polarity expressed in natural language and is typically categorized into three classes: positive, negative, and neutral. Although this task has continuously been widely explored, social media platforms, especially Twitter, bring up unique challenges. The language in social media is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and \#hashtags1[1].  In order to train and evaluate our project, we combine several publicly available tweet datasets, including Sentiment140, the Mass Twitter dataset, the Chirps dataset, and additional cleaned tweet corpora spanning multiple years and topics. Gathering these datasets can yield an extensive, heterogeneous collection suitable for both small- and large-scale experiments.

With the growing availability of large-scale Tweet datasets spanning several years and different topics, it has evolved increasingly critical to develop sentiment classification systems. These systems not only perform well on small, accurate datasets but also scale to enormous, heterogeneous corpora. Modern transformer-based language models, such as BERT and RoBERTa, have demonstrated powerful performance in many NLP tasks due to their contextual understanding. Classical lexicon-based tools, such as VADER, stay widely used for real-time or resource-constrained sentiment inference. In recent years, large-language models (LLMs), such as LLaMA, have made it feasible to fine-tune powerful generative architectures on domain-specific tasks, enabling improvements when working with massive datasets.

We run many experiments across datasets of different sizes to analyze how model performance changes with increasing dataset size. Using the combined Tweet datasets from several sources, we train and evaluate several models on consistent train, validation, and test splits. Our results show that transformer models outperform classical baselines, with RoBERTa gaining the strongest performance on medium-sized datasets. When scaling to the largest combined dataset, the LLaMA-based model gains the highest accuracy. Also, it demonstrates its advantage in capturing subtle sentiment features in large, diverse Tweet corpora. These findings emphasize the importance of both dataset scale and model capacity in Twitter sentiment classification. 


% \section{Task (5pt)}
% \todo{Define the task. -- Specify the inputs and outputs. This includes your notation. Section 2 of this paper~\cite{du-etal-2021-qa} may give you an idea.}


\section{Data (5pt)}

\todo{Describe the data you use, including how many examples are in the training, development, and test sets.}
In this project, we form a sentiment classification system using a massive combined Twitter corpus derived from multiple public datasets. We parsed and normalized data from Sentiment140, the Mass Twitter dataset, and the Chirps dataset. And additional labeled Tweet collections. After parsing and normalizing the datasets, we gather and unify all entries into a consistent three-class sentiment format, including positive, neutral, and negative. Then, we apply our preprocessing pipeline, which cleans noise, removes URLs, and standardizes tokens. In the meantime, we split the resulting corpus into three equal partitions. The final training set contains 168,747 tweets, and both the development set and test set have 168,747 tweets each. These amounts ensure balanced experimentation across all models. These split methods allow us to ensure balanced and consistent evaluation across all models used in the study.
\begin{table}[h!]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Dataset Split} & \textbf{Number of Examples} \\
\hline
Training Set     &  \texttt{168,747} \\
Development Set  &  \texttt{168,747} \\
Test Set         &  \texttt{168,747} \\
\hline
\end{tabular}
\caption{Number of examples in the training, development, and test sets.}
\label{tab:dataset_splits}
\end{table}


\section{Methodology (20pt)}

\red{If your project is proposing a new model/method.}

\todo{Describe your model. If you use a neural network, you should define the architecture. There's no need to write equations of common units (e.g., LSTM), so you can just use functions to refer to using such units. If you use linear models with features, provide details.}
% 
% Briefly describe what learning algorithm you are using. Specify your learning objective;
% 
% Briefly Describe your inference procedure. This is the process you use to make predictions during testing.}

\red{If your project is more analysis-based and propose how to analyze model behaviors/outputs}

\todo{Section 4 of \cite{das-etal-2022-automatic} is an example. If you do manual inspections, please provide the details such as analysis/annotation guidelines.}
In this section, we explain the data preparation pipeline, model training procedures, evaluation setup, and the manual analysis framework used to study and analyze model behaviors.  Our methodology follows the general structure of Section 4 in Das et al. (2022), which focuses on automated metrics and on qualitative inspection and error categorization to better understand and learn from the model outputs.

\subsection{Data Sources and Integration}

To create a general-purpose sentiment classification corpus, we aggregated four publicly available tweet-based datasets:

\begin{itemize} \small
    \item \textbf{Sentiment\_Analysis.csv}: A collection of labeled English posts annotated for positive, negative, or neutral sentiment.
    \item \textbf{Sentiment140.csv}: A large-scale Twitter dataset containing polarity-annotated tweets collected across multiple years.
    \item \textbf{Tweets.csv}: A mixed-source Twitter dataset containing short informal posts covering diverse subjects.
    \item \textbf{Smile.csv}: A sentiment dataset with three normalized labels (positive, neutral, negative).
\end{itemize}

Even though these datasets originate from different time periods and subject domains, their sentiment labels were standardized to a unified three-class scheme:

{\small
\begin{itemize}
    \item \textbf{Positive} -- 2
    \item \textbf{Neutral} -- 1
    \item \textbf{Negative} -- 0
\end{itemize}
}


This merging procedure allows us to form a wide, diverse, and domain-irrelevant sentiment corpus suitable for evaluating the robustness of modern language models.

\subsection{Data Preprocessing and Normalization}
\label{sec:preprocessing}

Before building the final train, validation, and test splits, all raw data files underwent a unified preprocessing stage:
{\small
\begin{itemize}
    \item \textbf{URL, hashtag, and username removal:}  
    Links such as \texttt{http://...} and user handles like \texttt{@name} are stripped using the \texttt{cleanLine} function.

    \item \textbf{Consistent text formatting:}  
    All text is converted to lowercase and cleaned from excessive whitespace or punctuation.

    \item \textbf{Character filtering:}  
    Non-English characters and redundant punctuation marks are removed or collapsed.

    \item \textbf{Label normalization:}  
    Dataset-specific labels (e.g., $-1$, $0$, $1$) are normalized to the range $0$--$2$ through a constant shift.
\end{itemize}
}

After preprocessing, each dataset is written in the format:
\[
\{ \text{sentence}, \text{label} \}.
\]

The \texttt{splitData()} routine is then executed to construct the final splits:
{\small
\begin{itemize}
    \item \textbf{80\% Training set:} \texttt{train.csv}
    \item \textbf{10\% Validation set:} \texttt{val.csv}
    \item \textbf{10\% Test set:} \texttt{test.csv}
\end{itemize}
}

This split guarantees that all models are trained and evaluated on the same unified distribution.

% ---------------------------------------------------------

\subsection{Model Architecture and Training Procedure}
\label{sec:models}

The project primarily experiments with transformer-based architectures due to their state-of-the-art performance in text classification. The implemented and planned models include:
{\small
\begin{itemize}
    \item \textbf{RoBERTa Twitter:}
    \texttt{cardiffnlp/\allowbreak twitter-roberta-base-sentiment-latest}

    \item \textbf{BERT-based sentiment models} from HuggingFace

    \item \textbf{(Planned)} LLaMA-based fine-tuning for comparison

    \item \textbf{(Planned)} VADER as a lexicon-based baseline
\end{itemize}
}

\subsubsection*{Training Pipeline}

All transformer models are fine-tuned using HuggingFace’s \texttt{Trainer} with the following defaults:
{\small
\begin{itemize}
    \item \textbf{Loss:} Cross-entropy
    \item \textbf{Optimizer:} AdamW
    \item \textbf{Batch size:} 8
    \item \textbf{Epochs:} 2--3
    \item \textbf{Evaluation metric:} Confusion matrix using \texttt{evaluate.load("confusion\_matrix")}
    \item \textbf{Tokenization:} \texttt{AutoTokenizer} with truncation to 256 tokens
\end{itemize}
}

The same train/validation/test splits are used for all models to ensure a fair comparison across different architectures.



\section{Implementations (15pt)}

\todo{Explain your implementation for the project (e.g. which commands/algorithms/data structures you used). What existing packages are used?
When describing your implementation details (e.g. decoding), we suggest that you include corresponding but only \textbf{important} code pieces (e.g. classes, data structures, algorithms. It may be in the form of a screenshot or latex source code like in Listing~\ref{lst:eg}).
}

\begin{listing}[ht]
\begin{minted}[frame=single,framesep=10pt]{python}
  import numpy  
  print("this is a piece of code")
\end{minted}
\caption{Example of a Code Piece.}
\label{lst:eg}
\end{listing}

\section{Experiments and Results (45pt)}

\red{If your project is proposing a new model/method.}
\paragraph{Development and Test Results}
\todo{Briefly explain here how you evaluated the models and metrics used. 
Put the results into clear tables or diagrams and include your observations and analysis.}

\paragraph{Error Analysis}
\todo{Qualitative analysis of selected failure examples. Show and discuss error examples from your development set. Identify certain classes of errors and use the examples to illustrate them. 
Please analyze with respect to unknown words as well.
}

\paragraph{(Bonus +2pt) Speed Analysis}
\todo{Speed analysis and computation needs are a major issue of the task. Makes sure to report the costs of inference per example.}

\red{If your project is more analysis-based and propose how to analyze model behaviors/outputs}

\todo{Please refer to Section 7 Experimental Results and Analysis in this paper, \url{https://aclanthology.org/2022.acl-long.274.pdf}. Basicly, you provide comprehensive analysis of model outputs, error cases, comparisons between model outputs and insights/obersations that can be derived.}

\subsection{Overall Model Performance}

Table~\ref{tab:overall_performance} summarizes the accuracy, macro-averaged
precision, recall, and F1 scores for all systems.  
The numerical values are imported from \texttt{table.tex}.

\begin{table*}[t]
\centering
\caption{Model Performance on Test Set}
\label{tab:overall_performance}
\begin{tabular}{lcccc}
\hline
\textbf{Model} & \textbf{Acc.} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} \\
\hline
BERT Multilingual & 0.7232 & 0.7529 & 0.7166 & 0.7252 \\
Sentiment-BERT & 0.7712 & 0.7974 & 0.7648 & 0.7737 \\
Twitter-RoBERTa & 0.7568 & 0.7728 & 0.7540 & 0.7604 \\
VADER & 0.6746 & 0.0244 & 0.0596 & 0.0346 \\
\hline
\end{tabular}
\end{table*}


Sentiment-BERT achieves the best overall results with 77.12\% accuracy and 0.7737
macro F1, outperforming both BERT Multilingual and Twitter-RoBERTa.
This aligns with expectations: Sentiment-BERT is explicitly trained for sentiment
classification, whereas BERT Multilingual supports broader multilingual tasks,
and Twitter-RoBERTa is optimized for tweet semantics but not solely sentiment.

\subsection{Class-Level Performance}

Across every model, the Neutral class consistently obtains the highest recall but the lowest precision. It reflects the dataset’s inherent ambiguity and the linguistic overlap between mild positive, mild negative, and neutral tweets.

\vspace{-1.2em}
\paragraph{BERT Multilingual}
{\small
\begin{itemize}
    \item Positive precision: 0.8599 (strongest)
    \item Negative recall: 0.6703 (weakest)
    \item Frequently mislabels mild sentiment as Neutral
    \item Confusion matrix reveals strong spillover from Positive $\rightarrow$ Neutral with 60 instances
\end{itemize}
}

\vspace{-1.2em}
\paragraph{Sentiment-BERT}
{\small
\begin{itemize}
    \item Highest precision, recall, and F1 across all classes
    \item Best detection of Neutral sentiment with Recall is 0.8432
    \item More robust separation between Positive and Negative
    \item Lower misclassification rate overall as shown in its confusion matrix
\end{itemize}
}

\vspace{-1.2em}
\paragraph{Twitter-RoBERTa}
{\small
\begin{itemize}
    \item Strong Positive Precision - 0.84
    \item Strong Negative Precision - 0.80
    \item More balanced performance than BERT Multilingual
    \item Slightly inferior to Sentiment-BERT on Neutral recall
    \item Tends to confuse borderline Negative/Neutral cases
\end{itemize}
}

These patterns imply that pretraining domain specificity, including Sentiment datasets vs. general-purpose vs. Twitter, corpora has a measurable impact on classification behaviour.

\subsection{Confusion Matrix Analysis}


\paragraph{BERT Multilingual}
{\small
\begin{itemize}
    \item 54 cases: Neutral mispredictions for Negative
    \item 60 cases: Neutral mispredictions for Positive
    \item Strong preference to classify ambiguous emotions as Neutral
    \item Implies conservative sentiment boundaries and difficulty with informal tone
\end{itemize}}

\vspace{-1.2em}
\paragraph{Sentiment-BERT}
{\small
\begin{itemize}
    \item Cleanest separation among classes
    \item Very low cross-polarity confusion, such as 6 cases: Positive $\rightarrow$ Negative
    \item Strong Neutral recall with 199 cases correct, indicating better understanding of ambiguous expressions
\end{itemize}}

\vspace{-1.2em}
\paragraph{Twitter-RoBERTa}
{\small
\begin{itemize}
    \item Balanced overall but less stable than Sentiment-BERT
    \item Higher Negative $\leftrightarrow$ Neutral swaps: 49 cases and 26 cases
    \item Robust Positive sentiment identification, with 156 cases correct
\end{itemize}}


\subsection{Error Analysis}

Common sources of error across all models include:
{\small
\begin{itemize}
    \item \textbf{Sarcasm and irony} \\
    ``yeah amazing job\ldots'' --- actually negative, but predicted as neutral.  
    Models lack explicit sarcasm-handling ability.

    \item \textbf{Implicit sentiment} \\
    ``wish the app worked better'' --- expresses negative intent without explicit emotional keywords.

    \item \textbf{Mixed sentiment} \\
    ``love the screen, hate the battery life''

    \item \textbf{Short or context-poor tweets} \\
    Single-token posts such as ``great'', ``wow'', ``ok'', where polarity depends heavily on external context.

    \item \textbf{Lexical ambiguity} \\
    Words like \textit{fine}, \textit{ok}, and \textit{sick} vary in meaning by speaker and domain.
\end{itemize}}

These patterns match prior findings in Twitter sentiment modeling literature and align with the analysis trends in Section~7 of the cited ACL 2022 paper.

\subsection{Comparison Across Models}

A consistent ranking emerges across all metrics:

{\small
\begin{itemize}
    \item \textbf{Sentiment-BERT (best)}
    \item \textbf{Twitter-RoBERTa}
    \item \textbf{BERT Multilingual (worst)}
\end{itemize}}

\vspace{-1.2em}
\paragraph{Interpretation:}

{\small
\begin{itemize}
    \item \textbf{Sentiment-BERT} benefits from pretraining directly on sentiment corpora, leading to superior generalization.
    \item \textbf{Twitter-RoBERTa} benefits from social media text modeling but lacks explicit polarity supervision.
    \item \textbf{BERT Multilingual} is trained on multilingual data, and its sentiment head is less aligned with informal, sarcastic, or domain-specific text.
\end{itemize}}

This demonstrates that \textbf{pretraining domain and task alignment are essential} for effective sentiment classification.

\subsection{Summary of Findings}

{\small
\begin{itemize}
    \item \textbf{Sentiment-BERT} provides the most accurate and stable performance, achieving \textbf{0.7712 accuracy} and \textbf{0.7737 macro F1}.
    \item \textbf{Twitter-RoBERTa} performs competitively but struggles with ambiguity around the Neutral boundary.
    \item \textbf{BERT Multilingual} displays the weakest polarity distinctions.
    \item The \textbf{Neutral class remains the primary source of error} across all models.
    \item \textbf{Dataset heterogeneity} increases robustness yet simultaneously introduces more ambiguity.
    \item Error patterns such as \textit{sarcasm, implicit sentiment, and mixed polarity} align with challenges documented in prior research.
\end{itemize}}



\section{Conclusion (5pt)}
\todo{Brief conclusion summarizing findings (from both numerical results and qualitative analysis).}

\bibliographystyle{plain}
\bibliography{custom}
[1] S. Rosenthal, P. Nakov, A. Ritter, and V. Stoyanov, “SemEval-2014 Task 9: Sentiment Analysis in Twitter,” 2014.

\end{document}

\documentclass[twocolumn]{article}
% \documentclass[a4paper]{article}
\usepackage{color}
\usepackage[top=1in,bottom=1in,left=1.2in,right=1.2in]{geometry}
\usepackage{hyperref}
\usepackage[small]{titlesec}

\usepackage{amsmath}

\usepackage[]{minted}
\newenvironment{code}{\captionsetup{type=listing}}

\newcommand{\todo}[1]{\textcolor{blue}{\textbf{TODO:} #1}}
\newcommand{\red}[1]{\textcolor{red}{\textbf{Note:} #1}}

\title{CS6320: Final Project Report \\ \begin{small}\url{https://github.com/COftedahl/cs6320_group_project}\end{small}}
\author{Brendan Martel \\ BXM240013
    \and Cole Oftedahl \\ CXO220001
    \and Bradley Evan Stover \\ BES170230
    \and Zhaotong Zhang \\ ZXZ220016}
\date{}

\begin{document}
\maketitle




\section{Introduction (10pt)}

\todo{The ﬁrst paragraph should brieﬂy describe task and data.}
\todo{The second paragraph should brieﬂy describe your approach. Try to motivate it.}
\todo{The third paragraph should describe your main experiments and results, including mentioning the data you use.}
Sentiment analysis aspires to determine the emotional polarity expressed in natural language and is typically categorized into three classes: positive, negative, and neutral. Although this task has continuously been widely explored, social media platforms, especially Twitter, bring up unique challenges. The language in social media is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and \#hashtags1[1].  In order to train and evaluate our project, we combine several publicly available tweet datasets, including Sentiment140, and two additional tweet corpora spanning multiple years and topics. Gathering these datasets can yield an extensive, heterogeneous collection suitable for both small- and large-scale experiments.

With the growing availability of large-scale Tweet datasets spanning several years and different topics, it has evolved increasingly critical to develop sentiment classification systems. These systems not only perform well on small, homogenious datasets but also scale to enormous, heterogeneous corpora. Modern transformer-based language models, such as BERT and RoBERTa, have demonstrated powerful performance in many NLP tasks due to their contextual understanding. Classical lexicon-based tools, such as VADER, stay widely used for real-time or resource-constrained sentiment inference. In recent years, large-language models (LLMs), such as LLaMA, have made it feasible to fine-tune powerful generative architectures on domain-specific tasks, enabling improvements when working with massive datasets.

We run many experiments across datasets of different sizes to analyze how model performance changes with increasing dataset size. Using the combined Tweet datasets from several sources, we train and evaluate several models on consistent train, validation, and test splits. Our results show that transformer models outperform classical baselines, with RoBERTa gaining the strongest performance on medium-sized datasets. When scaling to the largest combined dataset, the LLaMA-based model gains the highest accuracy. Also, it demonstrates its advantage in capturing subtle sentiment features in large, diverse Tweet corpora. These findings emphasize the importance of both dataset scale and model capacity in Twitter sentiment classification. 


% \section{Task (5pt)}
% \todo{Define the task. -- Specify the inputs and outputs. This includes your notation. Section 2 of this paper~\cite{du-etal-2021-qa} may give you an idea.}


\section{Data (5pt)}

\todo{Describe the data you use, including how many examples are in the training, development, and test sets.}
In this project, we form a sentiment classification system using a massive combined Twitter corpus derived from multiple public datasets. We parsed and normalized data from Sentiment140, Twitter Tweets Sentiment Dataset, Sentiment Analysis of Tweets dataset and additional labeled Tweet collections. After parsing and normalizing the datasets, we gather and unify all entries into a consistent three-class sentiment format, consisting of positive, neutral, and negative. Then, we apply our preprocessing pipeline, which cleans noise, removes URLs, and standardizes tokens. In the meantime, we split the resulting corpus into three equal partitions for training, validation, and testing. 
\begin{table}[h!]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Dataset Split} & \textbf{Number of Examples} \\
\hline
Training Set     &  \texttt{1,329,985} \\
Validation Set  &  \texttt{168,748} \\
Test Set         &  \texttt{168,748} \\
\hline
\end{tabular}
\caption{Number of examples in the training, Validation, and test sets.}
\label{tab:dataset_splits}
\end{table}


\section{Methodology (20pt)}

\red{If your project is proposing a new model/method.}

\todo{Describe your model. If you use a neural network, you should define the architecture. There's no need to write equations of common units (e.g., LSTM), so you can just use functions to refer to using such units. If you use linear models with features, provide details.}
% 
% Briefly describe what learning algorithm you are using. Specify your learning objective;
% 
% Briefly Describe your inference procedure. This is the process you use to make predictions during testing.}

\red{If your project is more analysis-based and propose how to analyze model behaviors/outputs}

\todo{Section 4 of \cite{das-etal-2022-automatic} is an example. If you do manual inspections, please provide the details such as analysis/annotation guidelines.}
In this section, we explain the data preparation pipeline, model training procedures, evaluation setup, and the manual analysis framework used to study and analyze model behaviors.  Our methodology follows the general structure of Section 4 in Das et al. (2022), which focuses on automated metrics and on qualitative inspection and error categorization to better understand and learn from the model outputs.

\subsection{Data Sources and Integration}

To create a general-purpose sentiment classification corpus, we aggregated three publicly available tweet-based datasets:

\begin{itemize} \small
    \item \textbf{Sentiment\_Analysis.csv}: A collection of labeled English posts annotated for positive, negative, or neutral sentiment.
    \item \textbf{Sentiment140.csv}: A large-scale Twitter dataset containing polarity-annotated tweets collected across multiple years.
    \item \textbf{Tweets.csv}: A mixed-source Twitter dataset containing short informal posts covering diverse subjects.
\end{itemize}

Even though these datasets originate from different time periods and subject domains, their sentiment labels were standardized to a unified three-class scheme:

{\small
\begin{itemize}
    \item \textbf{Positive} -- 2
    \item \textbf{Neutral} -- 1
    \item \textbf{Negative} -- 0
\end{itemize}
}


This merging procedure allows us to form a wide, diverse, and domain-agnostic sentiment corpus suitable for evaluating the robustness of modern language models.

\subsection{Data Preprocessing and Normalization}
\label{sec:preprocessing}

Before building the final train, validation, and test splits, all raw data files underwent a unified preprocessing stage:
{\small
\begin{itemize}
    \item \textbf{URL, hashtag, and username removal:}  
    Links such as \texttt{http://...} and user handles like \texttt{@name} are stripped using the \texttt{cleanLine} function @'s were replaced by a token.

    \item \textbf{Consistent text formatting:}  
    All text is converted to lowercase and cleaned from excessive whitespace or punctuation.

    \item \textbf{Character filtering:}  
    Non-English characters and redundant punctuation marks are removed or collapsed.

    \item \textbf{Label normalization:}  
    Dataset-specific labels (e.g., $-1$, $0$, $1$) are normalized to the range $0$--$2$ through a constant shift.
\end{itemize}
}

After preprocessing, each dataset is written in the format:
\[
\{ \text{sentence}, \text{label} \}.
\]

The \texttt{splitData()} routine is then executed to construct the final splits:
{\small
\begin{itemize}
    \item \textbf{80\% Training set:} \texttt{train.csv}
    \item \textbf{10\% Validation set:} \texttt{val.csv}
    \item \textbf{10\% Test set:} \texttt{test.csv}
\end{itemize}
}

This split guarantees that all models are trained and evaluated on the same unified distribution.

% ---------------------------------------------------------

\subsection{Model Architecture and Training Procedure}
\label{sec:models}

The project primarily experiments with transformer-based architectures due to their state-of-the-art performance in text classification. The implemented and planned models include:
{\small
\begin{itemize}
    \item \textbf{RoBERTa Twitter:}
    \texttt{cardiffnlp/\allowbreak twitter-roberta-base-sentiment-latest}

    \item \textbf{BERT-based sentiment models} from HuggingFace

    \item \textbf{(Planned)} LLaMA-based fine-tuning for comparison

    \item \textbf{VADER:} a lexicon-based baseline
\end{itemize}
}

\subsubsection*{Training Pipeline}

All transformer models are fine-tuned using HuggingFace’s \texttt{Trainer} with the following defaults:
{\small
\begin{itemize}
    \item \textbf{Loss:} Cross-entropy
    \item \textbf{Optimizer:} AdamW
    \item \textbf{Batch size:} 8
    \item \textbf{Epochs:} 2--3
    \item \textbf{Evaluation metric:} Confusion matrix using \texttt{evaluate.load("confusion\_matrix")}
    \item \textbf{Tokenization:} \texttt{AutoTokenizer} with truncation to 256 tokens
\end{itemize}
}

The same train/validation/test splits are used for all models to ensure a fair comparison across different architectures.

VADER, as a non-transformer, is used as a baseline against which to evaluate the transformer models. In this study, VADER is not trained, but is used to provide baseline classification scores by being run on the test dataset and evaluating its performance. 

\section{Implementations (15pt)}

\todo{Explain your implementation for the project (e.g. which commands/algorithms/data structures you used). What existing packages are used?
When describing your implementation details (e.g. decoding), we suggest that you include corresponding but only \textbf{important} code pieces (e.g. classes, data structures, algorithms. It may be in the form of a screenshot or latex source code like in Listing~\ref{lst:eg}).
}

The transformer models were implemented using the HuggingFace transformers library in python. Since multiple different models were evaluated, abstractions were used to allow simple reuse by passing a different parameter to indicate which model to use. 

To this end, the LanguageModel class was developed, incorporating a constructor method as well as train and test methods. Development of this class was largely based on the documentation available from HuggingFace, and thus the class closely follows example code from the provided tutorials. 

\begin{listing}[H]
\begin{minted}[frame=single,framesep=10pt]{python}
  def test(self, data): 
    return self.pipe(data)
\end{minted}
\caption{LanguageModel test() method}
\label{lst:eg}
\end{listing}

The constructor method simply accepts the model name as a parameter and then initializes variables to store the necessary model details used for training and testing. The test method is very simple, composed of only one line, listed above. The data parameter is the prompt being evaluated, and the simple interface provided by the transformers library allows calling pipe() with the data to compute the result using the loaded language model. 

\begin{listing}[H]
\begin{minted}[frame=single,framesep=10pt]{python}
def train(self, trainData, 
  trainingArgs
): 
  tokenizedTrainData = trainData.map(
    tokenizeDataset, batched=True)
  metric = evaluate.load(
    "confusion_matrix")
  trainer = Trainer(
    model=self.model,
    args=trainingArgs,
    train_dataset=tokenizedTrainData,
    eval_dataset=tokenizedTrainData,
    compute_metrics=compute_metrics,
  )
  trainer.train()
\end{minted}
\caption{LanguageModel train() method}
\label{lst:eg}
\end{listing}

The train method is more complex, requiring setup to utilize the provided Trainer interface from the transformers library. First, the method accepts the training dataset "trainData", and accepts "trainingArgs" to define parameters for the training. To implement the training, the Trainer class is used, which requires the training data to be tokenized and requires providing compute metrics, which allow the model to accurately compute the loss for each pass. 

\begin{listing}[H]
\begin{minted}[frame=single,framesep=10pt]{python}
  def tokenizeDataset(dataset):
    return self.tokenizer(
      dataset[TEXT_COLUMN_NAME], 
      padding="max_length", 
      truncation=True
    )
  tokenizedTrainData = trainData.map(
    tokenizeDataset, batched=True
  )
\end{minted}
\caption{Tokenizing data in train() method}
\label{lst:eg}
\end{listing}

The above listing details how the datasets are tokenized. This method is called to map "trainData" to "tokenizedTrainData", which ensures each input sent to the language model is of the proper size by using padding and truncation. 

\begin{listing}[H]
\begin{minted}[frame=single,framesep=10pt]{python}
  metric = evaluate.load(
    "confusion_matrix")

  def compute_metrics(eval_pred):
    logits, labels = eval_pred
    # convert the logits to 
    # their predicted class
    predictions = np.argmax(logits, 
      axis=-1)
    return metric.compute(
      predictions=predictions, 
      references=labels
    )
\end{minted}
\caption{Defining training metrics in train() method}
\label{lst:eg}
\end{listing}

Finally, the metrics must be provided to the Trainer to allow it to update the weights of the model properly. Thus, the above lists the evaluation methods used for training. Using the confusion matrix approach, the predictions are checked against the reference labels to provide loss in the training loop. 

The other portion of code written specifically for language processing tasks was the use of VADER. For this, the python package vaderSentiment was used, specifically utilizing the SentimentIntensityAnalyzer for computing a baseline sentiment prediction for the datasets. Much of this code is also based on the documentation for the vaderSentiment package. 

\begin{listing}[H]
\begin{minted}[frame=single,framesep=10pt]{python}
analyzer = SentimentIntensityAnalyzer()
predictions = []
for sentence in tqdm(trainingData[
    TEXT_COLUMN_NAME]):
  vs = analyzer.polarity_scores(
      sentence)
  label = getOverallSentiment(vs)
  predictions.append(label)
metrics = computeMetrics(
    predictions, trainingData)
\end{minted}
\caption{Main VADER Evaluation Loop}
\label{lst:eg}
\end{listing}

VADER provides most functionality already, with each call of the "polarity\_scores" method returning the probability of the given input belonging to each of the three possible classifications, along with an aggregration of those probabilities in a field called "compound". Thus, the program simply calls this method on each input sentence to get the classification, storing the results in an array to later be passed to a helper method for computing metrics of the model. However, since the "polarity\_scores" method returns an object with multiple classification scores, the method "getOverallSentiment" was developed to classify the sentiment using the returned compound score. 

\begin{listing}[H]
\begin{minted}[frame=single,framesep=10pt]{python}
THRESHOLD_VALUE = 0.5
POSITIVE_THRESHOLD = THRESHOLD_VALUE
NEGATIVE_THRESHOLD = -THRESHOLD_VALUE
def getOverallSentiment(sentiment): 
  if sentiment['compound'] >= 
      POSITIVE_THRESHOLD:
    return SENTIMENT_RESULT_ENUM.
        POSITIVE
  elif sentiment['compound'] <= 
      NEGATIVE_THRESHOLD:
    return SENTIMENT_RESULT_ENUM.
        NEGATIVE
  else:
    return SENTIMENT_RESULT_ENUM.
        NEUTRAL
\end{minted}
\caption{VADER Sentiment Classification}
\label{lst:eg}
\end{listing}

The above listing details the thresholds used for classifications, which are based on the thresholds used in the documentation. Based on the compound score, a negative score below the threshold is classified as negative sentiment, a positive score above the threshold is positive sentiment, and any other score is neutral. 

\section{Experiments and Results (45pt)}

\red{If your project is proposing a new model/method.}
\paragraph{Development and Test Results}
\todo{Briefly explain here how you evaluated the models and metrics used. 
Put the results into clear tables or diagrams and include your observations and analysis.}

\paragraph{Error Analysis}
\todo{Qualitative analysis of selected failure examples. Show and discuss error examples from your development set. Identify certain classes of errors and use the examples to illustrate them. 
Please analyze with respect to unknown words as well.
}

\paragraph{(Bonus +2pt) Speed Analysis}
\todo{Speed analysis and computation needs are a major issue of the task. Makes sure to report the costs of inference per example.}

\red{If your project is more analysis-based and propose how to analyze model behaviors/outputs}

\todo{Please refer to Section 7 Experimental Results and Analysis in this paper, \url{https://aclanthology.org/2022.acl-long.274.pdf}. Basicly, you provide comprehensive analysis of model outputs, error cases, comparisons between model outputs and insights/obersations that can be derived.}

\subsection{Overall Model Performance}

Table~\ref{tab:overall_performance} summarizes the accuracy, macro-averaged
precision, recall, and F1 scores for all systems.  
The numerical values are imported from \texttt{table.tex}.

\begin{table}[h!]
\raggedleft
\caption{Model Performance on Test Set}
\label{tab:overall_performance}
\begin{tabular}{lcccc}
\hline
\textbf{Model} & \textbf{Acc.} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} \\
\hline
BERT Multilingual & 0.7232 & 0.7529 & 0.7166 & 0.7252 \\
Sentiment-BERT & 0.7712 & 0.7974 & 0.7648 & 0.7737 \\
Twitter-RoBERTa & 0.7568 & 0.7728 & 0.7540 & 0.7604 \\
VADER & 0.6646 & 0.0219 & 0.0710 & 0.0335 \\
\hline
\end{tabular}
\end{table}


VADER performs the worst of all the models, which is expected as it does not follow the transformer architecture and because, owing to its purpose as a baseline, VADER is not trained on the Twitter data before testing, as the transformer models are. 
Sentiment-BERT achieves the best overall results with 77.12\% accuracy and 0.7737 macro F1, outperforming both BERT Multilingual and Twitter-RoBERTa.
This aligns with expectations: Sentiment-BERT is explicitly trained for sentiment
classification, whereas BERT Multilingual supports broader multilingual tasks,
and Twitter-RoBERTa is optimized for tweet semantics but not solely sentiment.

\subsection{Class-Level Performance}

Across all models, the Neutral class consistently shows the
highest recall but the lowest precision, reflecting dataset ambiguity and the
overlap among mild positive, mild negative, and neutral tweets.

\paragraph{BERT Multilingual}
{\small
\begin{itemize}
    \item Positive precision: 0.8599 (strongest)
    \item Negative recall: 0.6703 (weakest)
    \item Frequently mislabels mild sentiment as Neutral
    \item Confusion matrix reveals strong spillover from Positive $\rightarrow$ Neutral (60 instances)
\end{itemize}
}

\paragraph{Sentiment-BERT}
{\small
\begin{itemize}
    \item Highest precision, recall, and F1 across all classes
    \item Strong Neutral detection (Recall = 0.8432)
    \item Robust separation between Positive and Negative classes
    \item Lowest overall misclassification rate
\end{itemize}
}

\paragraph{Twitter-RoBERTa}
{\small
\begin{itemize}
    \item Strong Positive precision: 0.84
    \item Strong Negative precision: 0.80
    \item More balanced than BERT Multilingual
    \item Slightly weaker Neutral recall compared to Sentiment-BERT
    \item Tends to confuse borderline Negative/Neutral cases
\end{itemize}
}

These trends imply that pretraining domain specificity—sentiment-focused,
general-purpose, or Twitter-based—has a measurable effect on sentiment
classification behaviour.

\subsection{Confusion Matrix Analysis and Error Patterns}

\paragraph{BERT Multilingual}
The confusion matrix reveals systematic over-prediction of the Neutral class:
{\small
\begin{itemize}
    \item 54 cases of Negative $\rightarrow$ Neutral
    \item 60 cases of Positive $\rightarrow$ Neutral
    \item Only 7 cases of Neutral $\rightarrow$ Positive
\end{itemize}
}

This suggests a conservative decision boundary, likely due to multilingual
pretraining that lacks Twitter-specific cues such as sarcasm, ellipsis, or
informal intensifiers.

\paragraph{Sentiment-BERT}
{\small
\begin{itemize}
    \item Very low Positive $\rightarrow$ Negative confusion (6 cases)
    \item Strong polarity identification overall
    \item Occasional Negative $\rightarrow$ Neutral leakage (54 cases)
\end{itemize}}
Compared to BERT Multilingual, this model maintains a more balanced treatment of
all three sentiment labels.

\paragraph{Twitter-RoBERTa}
{\small
\begin{itemize}
    \item 49 cases of Negative $\rightarrow$ Neutral
    \item 26 cases of Neutral $\rightarrow$ Negative
    \item Strong Positive classification (156 correct)
    \item Struggles with sarcasm and subtle negativity despite Twitter-based pretraining
\end{itemize}}
This aligns with prior literature showing that social-media pretrained models
capture lexical variation well but require sentiment-specific training for finer
polarity distinctions.


\section{Conclusion (5pt)}
\todo{Brief conclusion summarizing findings (from both numerical results and qualitative analysis).}

\bibliographystyle{plain}
\bibliography{custom}
[1] S. Rosenthal, P. Nakov, A. Ritter, and V. Stoyanov, “SemEval-2014 Task 9: Sentiment Analysis in Twitter,” 2014.

\end{document}